{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLt7Kya8IyIE",
        "outputId": "ab054c7b-a6b9-4fa1-9325-77402e196410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6kSoa1OzCX5",
        "outputId": "ebe93246-5a3b-449a-a164-28ba56220f40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mtcnn\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (2.15.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python>=4.1.0->mtcnn) (1.25.2)\n",
            "Installing collected packages: mtcnn\n",
            "Successfully installed mtcnn-0.1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install mtcnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIAKUfPVyxj5"
      },
      "outputs": [],
      "source": [
        "import imutils\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from keras.models import load_model\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from mtcnn import MTCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6ivq-edzICo",
        "outputId": "fb494d22-7c56-4dec-d18d-c0df0ad0ca9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 198, 198, 32)      320       \n",
            "                                                                 \n",
            " average_pooling2d (Average  (None, 99, 99, 32)        0         \n",
            " Pooling2D)                                                      \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 97, 97, 64)        18496     \n",
            "                                                                 \n",
            " average_pooling2d_1 (Avera  (None, 48, 48, 64)        0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 46, 46, 128)       73856     \n",
            "                                                                 \n",
            " average_pooling2d_2 (Avera  (None, 23, 23, 128)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 21, 21, 256)       295168    \n",
            "                                                                 \n",
            " average_pooling2d_3 (Avera  (None, 10, 10, 256)       0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 256)               0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 132)               33924     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 7)                 931       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 422695 (1.61 MB)\n",
            "Trainable params: 422695 (1.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 100, 100, 1)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 98, 98, 70)        700       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 96, 96, 65)        41015     \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 96, 96, 65)        260       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 48, 48, 65)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 46, 46, 60)        35160     \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 46, 46, 60)        240       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 23, 23, 60)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 31740)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                2031424   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 40)                2600      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                1312      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 34        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2113273 (8.06 MB)\n",
            "Trainable params: 2113023 (8.06 MB)\n",
            "Non-trainable params: 250 (1000.00 Byte)\n",
            "_________________________________________________________________\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 48, 48, 1)]       0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 48, 48, 32)        320       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 48, 48, 32)        0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 48, 48, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 24, 24, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 6, 6, 256)         295168    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 6, 6, 256)         0         \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPoolin  (None, 3, 3, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2304)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               295040    \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 683267 (2.61 MB)\n",
            "Trainable params: 683267 (2.61 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from matplotlib import pyplot as plt # this lets you draw inline pictures in the notebooks\n",
        "import pylab # this allows you to control figure size\n",
        "pylab.rcParams['figure.figsize'] = (10.0, 8.0) # this controls figure size in the notebook\n",
        "\n",
        "export_dir='/content/drive/MyDrive/models/age_model_pretrained.h5'\n",
        "age_model = load_model(export_dir)\n",
        "\n",
        "# summarize model.\n",
        "age_model.summary()\n",
        "# load and evaluate a saved model\n",
        "export_dir='/content/drive/MyDrive/models/Copie de gender_model_pretrained.h5'\n",
        "gender_model = load_model(export_dir)\n",
        "\n",
        "# summarize model.\n",
        "gender_model.summary()\n",
        "export_dir='/content/drive/MyDrive/models/Copie de emotion_model_pretrained.h5'\n",
        "emotion_model = load_model(export_dir)\n",
        "\n",
        "# summarize model.\n",
        "emotion_model.summary()\n",
        "age_ranges = ['1-2', '3-9', '10-20', '21-27', '28-45', '46-65', '66-116']\n",
        "gender_ranges = ['male', 'female']\n",
        "emotion_ranges= ['positive','negative','neutral']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYE0sS8y0JI6"
      },
      "outputs": [],
      "source": [
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xNlPK2k2cMC"
      },
      "outputs": [],
      "source": [
        "face_detector = MTCNN()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "bsEZf6qO5iI6",
        "outputId": "c6508e50-a16c-4dcc-fc5c-2752bdbd1590"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function takePhoto(quality) {\n      const div = document.createElement('div');\n      const capture = document.createElement('button');\n      capture.textContent = 'Capture';\n      div.appendChild(capture);\n\n      const video = document.createElement('video');\n      video.style.display = 'block';\n      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n\n      document.body.appendChild(div);\n      div.appendChild(video);\n      video.srcObject = stream;\n      await video.play();\n\n      // Resize the output to fit the video element.\n      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n\n      // Wait for Capture to be clicked.\n      await new Promise((resolve) => capture.onclick = resolve);\n\n      const canvas = document.createElement('canvas');\n      canvas.width = video.videoWidth;\n      canvas.height = video.videoHeight;\n      canvas.getContext('2d').drawImage(video, 0, 0);\n      stream.getVideoTracks()[0].stop();\n      div.remove();\n      return canvas.toDataURL('image/jpeg', quality);\n    }\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "image_file = take_photo()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Gg4G1w-F0LM7",
        "outputId": "633ce247-f69a-47c1-d376-1170f9b418b7"
      },
      "outputs": [],
      "source": [
        "test_image = cv2.imread(image_file)\n",
        "gray = cv2.cvtColor(test_image,cv2.COLOR_BGR2GRAY)\n",
        "faces = face_detector.detect_faces(test_image)\n",
        "\n",
        "class_labels = emotion_ranges\n",
        "gender_labels = gender_ranges\n",
        "\n",
        "\n",
        "\n",
        "i = 0\n",
        "for face in faces:\n",
        "    if len(face['box']) == 4:\n",
        "        i = i + 1\n",
        "        x, y, w, h = face['box']\n",
        "        cv2.rectangle(test_image, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "        roi_gray = gray[y:y + h, x:x + w]\n",
        "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Get image ready for prediction\n",
        "        roi = roi_gray.astype('float') / 255.0  # Scale\n",
        "        roi = np.expand_dims(roi, axis=0)  # Expand dims to get it ready for prediction (1, 48, 48, 1)\n",
        "\n",
        "        emotion_img = cv2.resize(roi_gray, (48, 48), interpolation=cv2.INTER_AREA)\n",
        "        emotion_image_array = np.array(emotion_img)\n",
        "        emotion_input = np.expand_dims(emotion_image_array, axis=0)\n",
        "        output_emotion = class_labels[np.argmax(emotion_model.predict(emotion_input))]\n",
        "\n",
        "        gender_img = cv2.resize(roi_gray, (100, 100), interpolation=cv2.INTER_AREA)\n",
        "        gender_image_array = np.array(gender_img)\n",
        "        gender_input = np.expand_dims(gender_image_array, axis=0)\n",
        "        output_gender = gender_labels[np.argmax(gender_model.predict(gender_input))]\n",
        "\n",
        "        age_image = cv2.resize(roi_gray, (200, 200), interpolation=cv2.INTER_AREA)\n",
        "        age_input = age_image.reshape(-1, 200, 200, 1)\n",
        "        output_age = age_ranges[np.argmax(age_model.predict(age_input))]\n",
        "        output_str = str(i) + \": \" + output_gender + ', ' + output_age + ', ' + output_emotion\n",
        "        print(output_str)\n",
        "\n",
        "        col = (0, 255, 0)\n",
        "        cv2.putText(test_image, output_str, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.imshow(cv2.cvtColor(test_image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "\"\"\"Start webcam\n",
        "\n",
        "Click 'Capture' to make photo using your webcam.\n",
        "\n",
        "Read, resize and display the image.\n",
        "\n",
        "OpenCV’s deep learning face detector is based on the Single Shot Detector (SSD) framework with a ResNet base network. The network is defined and trained using the [Caffe Deep Learning framework](https://caffe.berkeleyvision.org/)\n",
        "\n",
        "Download the pre-trained face detection model, consisting of two files:\n",
        "\n",
        "- The network definition (deploy.prototxt)\n",
        "- The learned weights (res10_300x300_ssd_iter_140000.caffemodel)\n",
        "\n",
        "Load the pre-trained face detection network model from disk\n",
        "\n",
        "Use the [dnn.blobFromImage](https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/) function to construct an input blob by resizing the image to a fixed 300x300 pixels and then normalizing it.\n",
        "\n",
        "Pass the blob through the neural network and obtain the detections and predictions.\n",
        "\n",
        "Loop over the detections and draw boxes around the detected faces\n",
        "\n",
        "Show the resulting image\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
